{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the first part of code for Project 2.  It web scrapes Glassdoor of companies in Seatte that employ data analystics professionals.  Selenium was used as Glassdoor does not permit BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T20:53:12.092313Z",
     "start_time": "2018-07-10T20:53:08.221041Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import os\n",
    "chromedriver = \"/Users/ljborton/Downloads/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T20:59:24.849725Z",
     "start_time": "2018-07-10T20:59:23.423650Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "driver.get(\"https://www.glassdoor.com/index.htm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login with your personal info to get benefit info and click on notifications button.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below gives a list of all data analytics jobs in Seattle (if your default city is Seattle.)\n",
    "query = driver.find_element_by_xpath(\"//input[@id='sc.keyword']\")\n",
    "query.send_keys(\"Data Analytics\")\n",
    "query.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glassdoor will only allow you to page through 30 pages (30 jobs/page), then quits.  I grabbed the first 900 \n",
    "and called it \"list_of_nine\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loops though all 30 jobs on one page, then clicks to next page and repeats.  Page 30 is not scraped as it \n",
    "produces an error when it hits the \"next button\".  I could/should have fixed this.\n",
    "\n",
    "list_of_nine = []\n",
    "for i in range (1,30):\n",
    "    for j in range(1,31):        \n",
    "        job_xpath='//*[@id=\"MainCol\"]/div/ul/li[{}]/div[2]/div[2]'.format(j)\n",
    "        job_xpath = '//*[@id=\"MainCol\"]/div[1]/ul/li[{}]/div[2]/div[2]/div'.format(j)\n",
    "        job= driver.find_element_by_xpath(job_xpath).text \n",
    "        list_of_nine.append(job)\n",
    "    next_button_xpath='//*[@id=\"FooterPageNav\"]/div/ul/li[7]/a'  \n",
    "    next_button= driver.find_element_by_xpath(next_button_xpath)\n",
    "    next_button.click()\n",
    "time.sleep(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went back later and manually filtered by industry which gave less than 900 jobs each. I did not automate this \n",
    "as Glassdoor's filter did not  seem stable (didn't reset itself at times). I also noted the number of jobs \n",
    "listed per filter as it was needed to determine how the \"next page\" button would be coded.  In \n",
    "addition, the progress needs to be watched as popups would appear randomly.  After concatenating and removing \n",
    "duplicates, this list produced roughly the same companies; therefore I did not change from my \"list_of_nine\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# industry+num had to be input manually\n",
    "# industry_pg gives the number of full pages (30 jobs) and industry_rem gives the number of jobs on the last page.\n",
    "industry_pg=int(industry_num//30)\n",
    "industry_rem=industry_num%30\n",
    "\n",
    "''' The code below scrapes all the jobs on the first page, determine how the \"next page\" button is coded\n",
    "(if there is one), and continues scraping to the end of the list.  I should have incorporated the industry_rem\n",
    "for the last page as it now repeats the last job until it reaches 30 jobs.  I knew I would filter out duplicates\n",
    "though so it didn't really matter.\n",
    "''' \n",
    "\n",
    "list_of_datac = []\n",
    "for j in range(1,31):     \n",
    "    job_xpath = '//*[@id=\"MainCol\"]/div[1]/ul/li[{}]/div[2]/div[2]/div'.format(j)\n",
    "    job = driver.find_element_by_xpath(job_xpath).text \n",
    "    list_of_datac.append(job)\n",
    "    nextbox=0\n",
    "    if industry_num >30 and industry_num<=60:\n",
    "        nextbox = 2\n",
    "        next_button_xpath='//*[@id=\"FooterPageNav\"]/div/ul/li[4]/a'\n",
    "    elif industry_num > 60 and industry_num<=90:\n",
    "        nextbox = 3\n",
    "        next_button_xpath='//*[@id=\"FooterPageNav\"]/div/ul/li[5]/a'\n",
    "    elif industry_num >90 and industry_num<=120:\n",
    "        nextbox = 4\n",
    "        next_button_xpath='//*[@id=\"FooterPageNav\"]/div/ul/li[6]/a'\n",
    "    elif industry_num >120:\n",
    "        nextbox = 5\n",
    "        next_button_xpath='//*[@id=\"FooterPageNav\"]/div/ul/li[7]/a'\n",
    "    next_button= driver.find_element_by_xpath(next_button_xpath)\n",
    "    next_button.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle\n",
    "import pickle as pkl\n",
    "with open('list_of_nine.pkl', 'wb') as picklefile:\n",
    "    pkl.dump(list_of_nine, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Swisslog Healthcare – Kirkland, WA',\n",
       " 'FLEXE, Inc. – Seattle, WA',\n",
       " 'Robert Half – Seattle, WA',\n",
       " 'Amazon – Seattle, WA',\n",
       " 'Point B – Seattle, WA',\n",
       " 'Visionary Integration Professionals – Seattle, WA']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring in from pickle\n",
    "import pickle as pkl\n",
    "with open(\"list_of_nine.pkl\", 'rb') as picklefile: \n",
    "    list_of_nine = pkl.load(picklefile)\n",
    "list_of_nine[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n",
      "870\n"
     ]
    }
   ],
   "source": [
    "# Isolate the company only, remove city\n",
    "list_of_nine = [ x.split(\"–\")[0] for x in list_of_nine]\n",
    "print(len(list_of_nine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "#Many duplicate companies.  Remove them\n",
    "setnine=set(list_of_nine)\n",
    "print(len(setnine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The progress below also needs to be watched as the code fails and needs to restarted if it can't find a data object.\n",
    "I should have put a NoSuchElementException after each find_element_by_xpath..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Grabbing the data for each company in \"listnine\".  The code gets to the main page, clicks \n",
    "on the \"jobs/companies/salaries filter\" button, selects \"company\", inputs a company, directs to the company\n",
    "overview page (checks to make sure it isn't on it already), then gets to the company information page to scrape \n",
    "company data.  It tests to see if a star rating exists, and then goes to the company benefits page to grab \n",
    "benefit ratings.  It also tests to make sure a benefit rating exists.  Then repeats with the next company.\n",
    "'''\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "listnine=list(setnine)\n",
    "sizel=[]\n",
    "starl=[]\n",
    "headql=[]\n",
    "foundl=[]\n",
    "industryl=[]\n",
    "privatel=[]\n",
    "revenuel=[]\n",
    "ceol=[]\n",
    "benefitl=[]\n",
    "for names in setnine:\n",
    "    driver.get(\"https://www.glassdoor.com/index.htm\")\n",
    "    main_button_xpath='//*[@id=\"SiteSrchTop\"]/form/div/ul'\n",
    "    main_button = driver.find_element_by_xpath(main_button_xpath)\n",
    "    main_button.click()\n",
    "\n",
    "    main_company_button_xpath='//*[@id=\"SiteSrchTop\"]/form/div/ul/li[2]/span'\n",
    "    main_company_button = driver.find_element_by_xpath(main_company_button_xpath)\n",
    "    main_company_button.click()   \n",
    "    time.sleep(2)\n",
    "    \n",
    "    query_xpath = \"//input[@id='sc.keyword']\"\n",
    "    query = driver.find_element_by_xpath(query_xpath)\n",
    "    query.send_keys(names)\n",
    "    query.send_keys(Keys.RETURN)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    overview_xpath = '//*[@id=\"MainCol\"]/div[1]/div[2]/div[1]/div[2]/div[1]/a'\n",
    "       \n",
    "    try:\n",
    "        overview = driver.find_element_by_xpath(overview_xpath)\n",
    "        overview.click()\n",
    "        time.sleep(1)\n",
    "        headq_xpath= '//*[@id=\"EmpBasicInfo\"]/div[1]/div/div[2]'\n",
    "        headq=driver.find_element_by_xpath(headq_xpath).text\n",
    "        headql.append(headq)\n",
    "    except NoSuchElementException:\n",
    "        headq_xpath= '//*[@id=\"EmpBasicInfo\"]/div[1]/div/div[2]'\n",
    "        headq=driver.find_element_by_xpath(headq_xpath).text\n",
    "        headql.append(headq)\n",
    "          \n",
    "    size_xpath='//*[@id=\"EmpBasicInfo\"]/div[1]/div/div[3]/span'\n",
    "    size=driver.find_element_by_xpath(size_xpath).text\n",
    "    sizel.append(size)\n",
    "     \n",
    "    found_xpath='//*[@id=\"EmpBasicInfo\"]/div[1]/div/div[4]/span'\n",
    "    found=driver.find_element_by_xpath(found_xpath).text\n",
    "    foundl.append(found)\n",
    "\n",
    "    private_xpath='//*[@id=\"EmpBasicInfo\"]/div[1]/div/div[5]/span'\n",
    "    private =driver.find_element_by_xpath(private_xpath).text\n",
    "    privatel.append(private)\n",
    "                                  \n",
    "    industry_xpath = '//*[@id=\"EmpBasicInfo\"]/div[1]/div/div[6]/span'         \n",
    "    industry = driver.find_element_by_xpath(industry_xpath).text\n",
    "    industryl.append(industry)\n",
    "    \n",
    "    revenue_xpath='//*[@id=\"EmpBasicInfo\"]/div[1]/div/div[7]/span'\n",
    "    revenue = driver.find_element_by_xpath(revenue_xpath).text \n",
    "    revenuel.append(revenue)\n",
    "                                \n",
    "    ceo_xpath='//*[@id=\"EmpStats_Approve\"]'    \n",
    "    ceo=driver.find_element_by_xpath(ceo_xpath).text\n",
    "    ceol.append(ceo)\n",
    "    \n",
    "    star_xpath = '//*[@id=\"MainCol\"]/div[1]/div[2]/div[1]/div[2]/div[2]/div/span/span[1]'\n",
    "    \n",
    "    try:\n",
    "        star = driver.find_element_by_xpath(star_xpath).text\n",
    "        starlist.append(star)\n",
    "    except NoSuchElementException:\n",
    "        star= 'NA'\n",
    "        starlist.append(star)\n",
    "      \n",
    "   #click to benefits page to get benefits rating\n",
    "    ben_button_xpath='//*[@id=\"EmpLinksWrapper\"]/div/a[6]/span[1]'\n",
    "    ben = driver.find_element_by_xpath(ben_button_xpath)\n",
    "    ben.click()\n",
    "\n",
    "    benefit_xpath= '//*[@id=\"MainCol\"]/div[1]/div[1]/div[2]/div[1]'    \n",
    "    \n",
    "    try:\n",
    "        benefit = driver.find_element_by_xpath(benefit_xpath).text\n",
    "        benefitlist.append(star)\n",
    "    except NoSuchElementException:\n",
    "        benefit= 'NA'\n",
    "        benefitlist.append(benefit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''After error messages, it was found that some company pages had a \"part of\" field added that shifted\n",
    "some data into the wrong columns.  These rows were deleted.  They are shown with their index number.\n",
    "\n",
    "Accenture [40]\n",
    "Alibaba Group Holding [75]\n",
    "Murtech Consulting[101]\n",
    "SM Diversity [109]\n",
    "Monitor Deloitte listnine[82] \n",
    "RSM USLLP listnine [222]\n",
    "Tyler Technologies [245]\n",
    "Golock Global [290]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to get the list of al the companies that didn't have problems\n",
    "listc1= listnine [0:21]\n",
    "listc2= listnine [21:40]\n",
    "listc3= listnine [41:61]\n",
    "listc4= listnine [61:74]\n",
    "listc5= listnine [76:82]\n",
    "listc6= listnine [83:101]\n",
    "listc102= listnine [102:109]\n",
    "listc110= listnine [110:222]\n",
    "listc223= listnine [223:244]\n",
    "listc246= listnine [246:289]\n",
    "listc291= listnine [291:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the data\n",
    "ceo900 = (ceo1 +ceo2 + ceo3+ceo4+ceo5+ceo6+ceo102+ceo110+ceo223+ceo246+ceo7)\n",
    "star900 = (starlist1 + starlist2 + starlist3+starlist4+starlist5+starlist6+starlist102+starlist110+starlist223+starlist246+starlist7)\n",
    "benefit900 = (benefitlist1 + benefitlist2 + benefitlist3+benefitlist4+benefitlist5+benefitlist6+benefitlist102+benefitlist110+benefitlist223+benefitlist246+benefitlist7)\n",
    "revenue900 = (revenuelist1 + revenuelist2 +revenuelist3+revenuelist4+revenuelist5+revenuelist6+revenuelist102+revenuelist110+revenuelist223+revenuelist246+revenuelist7)\n",
    "industry900 = (industrylist1 + industrylist2 +industrylist3+industrylist4+industrylist5+industrylist6+industrylist102+industrylist110+industrylist223+industrylist246+industrylist7)\n",
    "size900 = (sizelist1 + sizelist2 +sizelist3+sizelist4+sizelist5+sizelist6+sizelist102+sizelist110+sizelist223+sizelist246+sizelist7)\n",
    "private900 = (privatelist1 + privatelist2 +privatelist3+privatelist4+privatelist5+privatelist6+privatelist102+privatelist110+privatelist223+privatelist246+privatelist7)\n",
    "headq900 = (headqlist1 + headqlist2 +headqlist3+headqlist4+headqlist5+headqlist6+headqlist102+headqlist110+headqlist223+headqlist246+headqlist7)\n",
    "found900 = (foundlist1 +foundlist4+foundlist5+foundlist6+foundlist102+foundlist110+foundlist223+foundlist246+foundlist7)\n",
    "company900 = (listc1+ listc2+ listc3+ listc4+listc5+ listc6+ listc102+listc110+ listc223 + listc246 + listc291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df900 = pd.DataFrame(\n",
    "    {'Company': company900, 'Reviews': star900, 'CEO_Reviews': ceo900,'Founded': found900, 'Headquarters':headq900, \n",
    "     'Private_Public' : private900, 'Benefit_Rating':benefit900, 'Revenue': revenue900, \n",
    "     'Industry': industry900, 'Size': size900})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle\n",
    "import pickle as pkl\n",
    "with open('df900.pkl', 'wb') as picklefile:\n",
    "    pkl.dump(list_of_nine, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
