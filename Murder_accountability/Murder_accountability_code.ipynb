{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting which Murders will be Unsolved using Murder Accountability Project data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import patsy\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.figure\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import recall_score, classification_report, roc_curve, auc, \\\n",
    "accuracy_score,f1_score, confusion_matrix, precision_recall_curve\n",
    "\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in all of the data and then creating a 10% random sample to make a more manageable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752313, 34)\n",
      "(75231, 34)\n"
     ]
    }
   ],
   "source": [
    "doc = codecs.open('Year_all.csv','rU','UTF-16') \n",
    "dfall = pd.read_csv(doc, sep='\\t')\n",
    "dfsample = dfall.sample(frac=0.1, replace=True)\n",
    "print(dfall.shape)\n",
    "print(dfsample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing:    \n",
    "Replace all unknows with NaN         \n",
    "Remove unecssary columns.  This includes indices, file date, etc.       \n",
    "The \"offender\" columns and relationship columns are removed because when predicting unsolved murders nothing is known about the offender.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfsample.replace(999, np.nan)\n",
    "df = df.replace (\"Unknown\", np.nan)\n",
    "df = df.replace (\"Unknown or not reported\", np.nan)\n",
    "df = df.drop(['Action Type','Agency','Cntyfips','Calculation2', 'Circumstance','File Date', 'Homicide', \n",
    "              'ID', 'Incident' ,'Ori', 'Off Age','Off Count', 'Off Sex', 'Off Ethnic', 'Off Race', \n",
    "              'Relationship','Situation','Source','State Name','State', 'Vic Ethnic', 'Subcircum','Fstate', \n",
    "              'Number of Records'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrow down the MSAs by selecting the one with the most value counts and the ones contributing most to the prediction.               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New York-New Jersey-Long Island, NY-NJ          5896\n",
       "Los Angeles-Long Beach, CA                      5403\n",
       "Chicago-Naperville-Joliet, IL-IN-WI             3288\n",
       "Detroit-Warren-Livonia, MI                      2398\n",
       "Houston-Sugar Land-Baytown, TX                  2142\n",
       "Philadelphia-Camden-Wilmington, PA-NJ-DE        2117\n",
       "Dallas-Fort Worth-Arlington, TX                 1894\n",
       "Miami-Fort Lauderdale, FL                       1750\n",
       "Washington-Arlington-Alexandria, DC-VA-MD-WV    1465\n",
       "San Francisco-Oakland-Fremont, CA               1387\n",
       "Atlanta-Sandy Springs-Marietta, GA              1387\n",
       "Baltimore-Towson, MD                            1280\n",
       "New Orleans-Metairie-Kenner, LA                 1123\n",
       "St. Louis, MO-IL                                1089\n",
       "Phoenix-Mesa-Scottsdale, AZ                     1003\n",
       "Riverside-San Bernardino, CA                     979\n",
       "Rural North Carolina                             726\n",
       "Rural Texas                                      708\n",
       "Memphis, TN-MS-AR                                675\n",
       "Cleveland-Elyria-Mentor, OH                      661\n",
       "San Antonio, TX                                  633\n",
       "San Diego-Carlsbad-San Marcos, CA                579\n",
       "Kansas City, MO-KS                               577\n",
       "Virginia Beach-Norfolk-Newport News, VA-NC       575\n",
       "Tampa-St. Petersburg-Clearwater, FL              567\n",
       "Boston-Cambridge-Quincy, MA-NH                   529\n",
       "Birmingham-Hoover, AL                            507\n",
       "Seattle-Tacoma-Bellevue, WA                      493\n",
       "Sacramento-Roseville, CA                         486\n",
       "Indianapolis, IN                                 483\n",
       "                                                ... \n",
       "Pittsfield, MA                                     8\n",
       "Elmira, NY                                         8\n",
       "Fargo, ND-MN                                       8\n",
       "Appleton, WI                                       8\n",
       "Janesville, WI                                     8\n",
       "Punta Gorda, FL                                    8\n",
       "Missoula, MT                                       7\n",
       "Fairbanks, AK                                      7\n",
       "Glens Falls, NY                                    7\n",
       "Iowa City, IA                                      7\n",
       "Danville, IL                                       7\n",
       "Rapid City, SD                                     6\n",
       "Bloomington-Normal, IL                             6\n",
       "Lawrence, KS                                       6\n",
       "Bismarck, ND                                       6\n",
       "Corvallis, OR                                      5\n",
       "Grand Forks, ND-MN                                 5\n",
       "St. Cloud, MN                                      5\n",
       "State College, PA                                  5\n",
       "Lewiston, ID-WA                                    5\n",
       "Dubuque, IA                                        5\n",
       "Oshkosh-Neenah, WI                                 4\n",
       "Wausau, WI                                         3\n",
       "Eau Claire, WI                                     3\n",
       "Ames, IA                                           3\n",
       "Logan, UT-ID                                       3\n",
       "Sheboygan, WI                                      2\n",
       "Columbus, IN                                       2\n",
       "St. George, UT                                     1\n",
       "Carson City, NV                                    1\n",
       "Name: MSA, Length: 407, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['MSA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= df[['MSA', 'Solved']]\n",
    "df4 = df4.dropna()\n",
    "msa=patsy.dmatrix('MSA',data=df4,return_type='dataframe')\n",
    "allt = df4.join(msa)\n",
    "allt= allt.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allt.iloc[:,3:]\n",
    "y = allt['Solved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msa</th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>MSA[T.New York-New Jersey-Long Island, NY-NJ]</td>\n",
       "      <td>0.160219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>MSA[T.Los Angeles-Long Beach, CA]</td>\n",
       "      <td>0.097195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>MSA[T.Washington-Arlington-Alexandria, DC-VA-M...</td>\n",
       "      <td>0.069587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>MSA[T.Chicago-Naperville-Joliet, IL-IN-WI]</td>\n",
       "      <td>0.051981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>MSA[T.St. Louis, MO-IL]</td>\n",
       "      <td>0.033412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MSA[T.Baltimore-Towson, MD]</td>\n",
       "      <td>0.031381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>MSA[T.San Francisco-Oakland-Fremont, CA]</td>\n",
       "      <td>0.023682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>MSA[T.New Orleans-Metairie-Kenner, LA]</td>\n",
       "      <td>0.023409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>MSA[T.Memphis, TN-MS-AR]</td>\n",
       "      <td>0.017181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>MSA[T.Detroit-Warren-Livonia, MI]</td>\n",
       "      <td>0.013845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>MSA[T.Miami-Fort Lauderdale, FL]</td>\n",
       "      <td>0.013051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MSA[T.Boston-Cambridge-Quincy, MA-NH]</td>\n",
       "      <td>0.011847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>MSA[T.Dallas-Fort Worth-Arlington, TX]</td>\n",
       "      <td>0.011823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>MSA[T.Rural South Carolina]</td>\n",
       "      <td>0.011431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MSA[T.Buffalo-Niagara Falls, NY]</td>\n",
       "      <td>0.010468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>MSA[T.Rural Louisiana]</td>\n",
       "      <td>0.009659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>MSA[T.Rural Virginia]</td>\n",
       "      <td>0.009382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>MSA[T.Rural Texas]</td>\n",
       "      <td>0.009014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>MSA[T.San Diego-Carlsbad-San Marcos, CA]</td>\n",
       "      <td>0.008980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>MSA[T.Rural Mississippi]</td>\n",
       "      <td>0.008669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>MSA[T.Rural West Virginia]</td>\n",
       "      <td>0.007736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>MSA[T.Las Vegas-Paradise, NV]</td>\n",
       "      <td>0.007704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MSA[T.Atlanta-Sandy Springs-Marietta, GA]</td>\n",
       "      <td>0.007223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>MSA[T.Rural Georgia]</td>\n",
       "      <td>0.007113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MSA[T.Austin-Round Rock, TX]</td>\n",
       "      <td>0.006503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>MSA[T.Stockton, CA]</td>\n",
       "      <td>0.006453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>MSA[T.Riverside-San Bernardino, CA]</td>\n",
       "      <td>0.005635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>MSA[T.Hartford, CT]</td>\n",
       "      <td>0.005224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>MSA[T.Rural Tennessee]</td>\n",
       "      <td>0.005213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>MSA[T.Fresno, CA]</td>\n",
       "      <td>0.005075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSA[T.Akron, OH]</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>MSA[T.Janesville, WI]</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>MSA[T.Kennewick-Richland-Pasco, WA]</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>MSA[T.Longview, WA]</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>MSA[T.Mansfield, OH]</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>MSA[T.Lakeland, FL]</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>MSA[T.Duluth, MN-WI]</td>\n",
       "      <td>0.000117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>MSA[T.Santa Fe, NM]</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>MSA[T.Colorado Springs, CO]</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>MSA[T.Huntington-Ashland, WV-KY-OH]</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>MSA[T.Gulfport-Biloxi, MS]</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>MSA[T.Parkersburg-Marietta-Vienna, WV-OH]</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>MSA[T.Tyler, TX]</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>MSA[T.Fond du Lac, WI]</td>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>MSA[T.Lewiston-Auburn, ME]</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MSA[T.Beaumont-Port Arthur, TX]</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>MSA[T.Coeur d Alene, ID]</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>MSA[T.Idaho Falls, ID]</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>MSA[T.Wenatchee, WA]</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MSA[T.Ames, IA]</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>MSA[T.Harrisonburg, VA]</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>MSA[T.Rural Arizona]</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>MSA[T.St. George, UT]</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>MSA[T.Yuba City, CA]</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>MSA[T.Sheboygan, WI]</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>MSA[T.Cumberland, MD-WV]</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>MSA[T.Logan, UT-ID]</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>MSA[T.Iowa City, IA]</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>MSA[T.Fargo, ND-MN]</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>MSA[T.Corvallis, OR]</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   msa      Coef\n",
       "231      MSA[T.New York-New Jersey-Long Island, NY-NJ]  0.160219\n",
       "196                  MSA[T.Los Angeles-Long Beach, CA]  0.097195\n",
       "390  MSA[T.Washington-Arlington-Alexandria, DC-VA-M...  0.069587\n",
       "62          MSA[T.Chicago-Naperville-Joliet, IL-IN-WI]  0.051981\n",
       "364                            MSA[T.St. Louis, MO-IL]  0.033412\n",
       "23                         MSA[T.Baltimore-Towson, MD]  0.031381\n",
       "337           MSA[T.San Francisco-Oakland-Fremont, CA]  0.023682\n",
       "230             MSA[T.New Orleans-Metairie-Kenner, LA]  0.023409\n",
       "207                           MSA[T.Memphis, TN-MS-AR]  0.017181\n",
       "90                   MSA[T.Detroit-Warren-Livonia, MI]  0.013845\n",
       "209                   MSA[T.Miami-Fort Lauderdale, FL]  0.013051\n",
       "40               MSA[T.Boston-Cambridge-Quincy, MA-NH]  0.011847\n",
       "79              MSA[T.Dallas-Fort Worth-Arlington, TX]  0.011823\n",
       "317                        MSA[T.Rural South Carolina]  0.011431\n",
       "47                    MSA[T.Buffalo-Niagara Falls, NY]  0.010468\n",
       "297                             MSA[T.Rural Louisiana]  0.009659\n",
       "323                              MSA[T.Rural Virginia]  0.009382\n",
       "320                                 MSA[T.Rural Texas]  0.009014\n",
       "336           MSA[T.San Diego-Carlsbad-San Marcos, CA]  0.008980\n",
       "303                           MSA[T.Rural Mississippi]  0.008669\n",
       "325                         MSA[T.Rural West Virginia]  0.007736\n",
       "183                      MSA[T.Las Vegas-Paradise, NV]  0.007704\n",
       "17           MSA[T.Atlanta-Sandy Springs-Marietta, GA]  0.007223\n",
       "289                               MSA[T.Rural Georgia]  0.007113\n",
       "21                        MSA[T.Austin-Round Rock, TX]  0.006503\n",
       "366                                MSA[T.Stockton, CA]  0.006453\n",
       "273                MSA[T.Riverside-San Bernardino, CA]  0.005635\n",
       "139                                MSA[T.Hartford, CT]  0.005224\n",
       "319                             MSA[T.Rural Tennessee]  0.005213\n",
       "119                                  MSA[T.Fresno, CA]  0.005075\n",
       "..                                                 ...       ...\n",
       "0                                     MSA[T.Akron, OH]  0.000126\n",
       "159                              MSA[T.Janesville, WI]  0.000125\n",
       "168                MSA[T.Kennewick-Richland-Pasco, WA]  0.000125\n",
       "195                                MSA[T.Longview, WA]  0.000119\n",
       "204                               MSA[T.Mansfield, OH]  0.000118\n",
       "178                                MSA[T.Lakeland, FL]  0.000118\n",
       "94                                MSA[T.Duluth, MN-WI]  0.000117\n",
       "343                                MSA[T.Santa Fe, NM]  0.000114\n",
       "70                         MSA[T.Colorado Springs, CO]  0.000113\n",
       "148                MSA[T.Huntington-Ashland, WV-KY-OH]  0.000111\n",
       "134                         MSA[T.Gulfport-Biloxi, MS]  0.000108\n",
       "247          MSA[T.Parkersburg-Marietta-Vienna, WV-OH]  0.000107\n",
       "379                                   MSA[T.Tyler, TX]  0.000106\n",
       "114                             MSA[T.Fond du Lac, WI]  0.000105\n",
       "188                         MSA[T.Lewiston-Auburn, ME]  0.000097\n",
       "29                     MSA[T.Beaumont-Port Arthur, TX]  0.000088\n",
       "68                            MSA[T.Coeur d Alene, ID]  0.000087\n",
       "150                             MSA[T.Idaho Falls, ID]  0.000086\n",
       "394                               MSA[T.Wenatchee, WA]  0.000083\n",
       "8                                      MSA[T.Ames, IA]  0.000073\n",
       "138                            MSA[T.Harrisonburg, VA]  0.000072\n",
       "282                               MSA[T.Rural Arizona]  0.000071\n",
       "362                              MSA[T.St. George, UT]  0.000070\n",
       "406                               MSA[T.Yuba City, CA]  0.000070\n",
       "349                               MSA[T.Sheboygan, WI]  0.000068\n",
       "78                            MSA[T.Cumberland, MD-WV]  0.000058\n",
       "193                                MSA[T.Logan, UT-ID]  0.000044\n",
       "152                               MSA[T.Iowa City, IA]  0.000042\n",
       "106                                MSA[T.Fargo, ND-MN]  0.000039\n",
       "77                                MSA[T.Corvallis, OR]  0.000033\n",
       "\n",
       "[408 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "randomforest = RandomForestClassifier()\n",
    "randomforest.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = randomforest.predict(X_train)\n",
    "y_pred_test = randomforest.predict(X_test)\n",
    "\n",
    "table = list(zip(X,randomforest.feature_importances_))\n",
    "dftable = pd.DataFrame(table, columns=['msa','Coef'])\n",
    "dftable.sort_values(by='Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns to remove spaces     \n",
    "Assign binary categories to 0, 1     \n",
    "Select important cities and make into dummy variables. Group all others and set to \"other\"     \n",
    "Make month, race, and weapon into dummy categories.    \n",
    "Assign other categories that are heavily weighted to one value to 1, the others to 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns= [\n",
    "'Agentype','MSA', 'Month', 'Solved', \n",
    "'Vic_Age', 'Vic_Count' ,'Vic_Race', 'Vic_Sex', 'Weapon', 'Year']\n",
    "df['Solved'] = df['Solved'].map({'Yes': 1, 'No': 0})\n",
    "df['vic_sex'] = df['Vic_Sex'].map({'Female': 1, 'Male': 0})\n",
    "df['agent_type'] = np.where(df['Agentype']=='Municipal police', 1, 0)\n",
    "list_of_cities4 = [\n",
    "'New York-New Jersey-Long Island, NY-NJ',\n",
    "'Los Angeles-Long Beach, CA',                      \n",
    "'Houston-Sugar Land-Baytown, TX',                  \n",
    "'Detroit-Warren-Livonia, MI',                      \n",
    "'Dallas-Fort Worth-Arlington, TX',                 \n",
    "'Philadelphia-Camden-Wilmington, PA-NJ-DE',        \n",
    "'Washington-Arlington-Alexandria, DC-VA-MD-WV',   \n",
    "'Chicago-Naperville-Joliet, IL-IN-WI',              \n",
    "'San Francisco-Oakland-Fremont, CA' ,               \n",
    "'Baltimore-Towson, MD' ,                           \n",
    "'Miami-Fort Lauderdale, FL',                       \n",
    "'Atlanta-Sandy Springs-Marietta, GA',               \n",
    "'New Orleans-Metairie-Kenner, LA',                 \n",
    "'Riverside-San Bernardino, CA',                  \n",
    "'St. Louis, MO-IL',\n",
    "'Boston-Cambridge-Quincy, MA-NH',\n",
    "'Birmingham-Hoover, AL',\n",
    "'Bridgeport-Stamford-Norwalk, CT',\n",
    "'Memphis, TN-MS-AR']  \n",
    "\n",
    "def city_other(city):\n",
    "    if city in list_of_cities4:\n",
    "        return city\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['msa']= df['MSA'].apply(city_other)\n",
    "\n",
    "df=df.dropna()\n",
    "msa=patsy.dmatrix('msa',data=df,return_type='dataframe')\n",
    "month=patsy.dmatrix('Month',data=df,return_type='dataframe')\n",
    "vicrace=patsy.dmatrix('Vic_Race',data=df,return_type='dataframe')\n",
    "weapon=patsy.dmatrix('Weapon',data=df,return_type='dataframe')\n",
    "\n",
    "df1 = pd.concat([df, msa], axis = 1)\n",
    "df3 = pd.concat([df1, month], axis = 1)\n",
    "df4 = pd.concat([df3, vicrace], axis = 1)\n",
    "df5 = pd.concat([df4, weapon], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df5 is the final dataframe to use for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Murder Accountability Project has a nice website with tableau dashboards  (http://murderdata.org/)\n",
    "A lot of EDA was done interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe X with all relevant features as columns.  Original columns were not removed.        \n",
    "Create target dataframe y     \n",
    "Create training and testing data sets.    \n",
    "Scale data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df5[[ \n",
    "'Vic_Age', 'Vic_Count', 'Year',\n",
    "'vic_sex', 'agent_type', \n",
    "'msa[T.Baltimore-Towson, MD]',\n",
    "'msa[T.Birmingham-Hoover, AL]', 'msa[T.Boston-Cambridge-Quincy, MA-NH]',\n",
    "'msa[T.Bridgeport-Stamford-Norwalk, CT]',\n",
    "'msa[T.Chicago-Naperville-Joliet, IL-IN-WI]',\n",
    "'msa[T.Dallas-Fort Worth-Arlington, TX]',\n",
    "'msa[T.Detroit-Warren-Livonia, MI]',\n",
    "'msa[T.Houston-Sugar Land-Baytown, TX]',\n",
    "'msa[T.Los Angeles-Long Beach, CA]', 'msa[T.Memphis, TN-MS-AR]',\n",
    "'msa[T.Miami-Fort Lauderdale, FL]',\n",
    "'msa[T.New Orleans-Metairie-Kenner, LA]',\n",
    "'msa[T.New York-New Jersey-Long Island, NY-NJ]', 'msa[T.Other]',\n",
    "'msa[T.Philadelphia-Camden-Wilmington, PA-NJ-DE]',\n",
    "'msa[T.Riverside-San Bernardino, CA]',\n",
    "'msa[T.San Francisco-Oakland-Fremont, CA]', 'msa[T.St. Louis, MO-IL]',\n",
    "'msa[T.Washington-Arlington-Alexandria, DC-VA-MD-WV]',   \n",
    "'Month[T.August]', 'Month[T.December]', 'Month[T.February]',\n",
    "'Month[T.January]', 'Month[T.July]', 'Month[T.June]', 'Month[T.March]',\n",
    "'Month[T.May]', 'Month[T.November]', 'Month[T.October]',\n",
    "'Month[T.September]', \n",
    "'Vic_Race[T.Asian or Pacific Islander]', 'Vic_Race[T.Black]',\n",
    "'Vic_Race[T.White]', \n",
    "'Weapon[T.Blunt object - hammer, club, etc]', 'Weapon[T.Drowning]',\n",
    "'Weapon[T.Explosives]', 'Weapon[T.Fire]',\n",
    "'Weapon[T.Firearm, type not stated]',\n",
    "'Weapon[T.Handgun - pistol, revolver, etc]',\n",
    "'Weapon[T.Knife or cutting instrument]',\n",
    "'Weapon[T.Narcotics or drugs, sleeping pills]', 'Weapon[T.Other gun]',\n",
    "'Weapon[T.Other or type unknown]',\n",
    "'Weapon[T.Personal weapons, includes beating]',\n",
    "'Weapon[T.Poison - does not include gas]',\n",
    "'Weapon[T.Pushed or thrown out window]', 'Weapon[T.Rifle]',\n",
    "'Weapon[T.Shotgun]', 'Weapon[T.Strangulation - hanging]']]\n",
    "        \n",
    "y = df5['Solved']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "The most important metric in this project was recall.   \n",
    "Training and testing accuracy were output to validate there was no overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train)\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print(classification_report (y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knn algorithm was not optimized because    \n",
    "1.  It takes a very long time to run    \n",
    "2. There was a very low probability this would be the best algorithm.    \n",
    "To optimize it, the following hyperparameters would be tested:           \n",
    "k_list = [10, 50, 100]    \n",
    "weight_options = ['uniform', 'distance']     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  892  5350]\n",
      " [  702 15076]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.14      0.23      6242\n",
      "          1       0.74      0.96      0.83     15778\n",
      "\n",
      "avg / total       0.69      0.73      0.66     22020\n",
      "\n",
      "Train Accuracy: 0.7192487349163098\n",
      "Test Accuracy: 0.7251589464123525\n"
     ]
    }
   ],
   "source": [
    "lr_model_all = LogisticRegression()\n",
    "lr_model_all.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr_model_all.predict(X_train)\n",
    "y_pred_test = lr_model_all.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print(classification_report (y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.01, 'penalty': 'l2'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.648 (+/-0.038) for {'C': 0.01, 'penalty': 'l1'}\n",
      "0.651 (+/-0.030) for {'C': 0.01, 'penalty': 'l2'}\n",
      "0.650 (+/-0.030) for {'C': 0.1, 'penalty': 'l1'}\n",
      "0.651 (+/-0.030) for {'C': 0.1, 'penalty': 'l2'}\n",
      "0.651 (+/-0.029) for {'C': 1, 'penalty': 'l1'}\n",
      "0.651 (+/-0.030) for {'C': 1, 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.14      0.23      6242\n",
      "          1       0.74      0.96      0.83     15778\n",
      "\n",
      "avg / total       0.69      0.73      0.66     22020\n",
      "\n",
      "[[  892  5350]\n",
      " [  702 15076]]\n",
      "\n",
      "Train Accuracy: 0.7194433631763332\n",
      "Test Accuracy: 0.7251589464123525\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.1, 'penalty': 'l2'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.539 (+/-0.009) for {'C': 0.01, 'penalty': 'l1'}\n",
      "0.550 (+/-0.010) for {'C': 0.01, 'penalty': 'l2'}\n",
      "0.549 (+/-0.009) for {'C': 0.1, 'penalty': 'l1'}\n",
      "0.550 (+/-0.010) for {'C': 0.1, 'penalty': 'l2'}\n",
      "0.550 (+/-0.010) for {'C': 1, 'penalty': 'l1'}\n",
      "0.550 (+/-0.010) for {'C': 1, 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.14      0.23      6242\n",
      "          1       0.74      0.96      0.83     15778\n",
      "\n",
      "avg / total       0.69      0.73      0.66     22020\n",
      "\n",
      "[[  892  5350]\n",
      " [  700 15078]]\n",
      "\n",
      "Train Accuracy: 0.7192487349163098\n",
      "Test Accuracy: 0.7252497729336966\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.1, 'penalty': 'l2'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.509 (+/-0.014) for {'C': 0.01, 'penalty': 'l1'}\n",
      "0.529 (+/-0.014) for {'C': 0.01, 'penalty': 'l2'}\n",
      "0.527 (+/-0.013) for {'C': 0.1, 'penalty': 'l1'}\n",
      "0.530 (+/-0.014) for {'C': 0.1, 'penalty': 'l2'}\n",
      "0.529 (+/-0.014) for {'C': 1, 'penalty': 'l1'}\n",
      "0.529 (+/-0.014) for {'C': 1, 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.14      0.23      6242\n",
      "          1       0.74      0.96      0.83     15778\n",
      "\n",
      "avg / total       0.69      0.73      0.66     22020\n",
      "\n",
      "[[  892  5350]\n",
      " [  700 15078]]\n",
      "\n",
      "Train Accuracy: 0.7192487349163098\n",
      "Test Accuracy: 0.7252497729336966\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "degree = [1,2,3]\n",
    "scores = ['recall']\n",
    "hyperparameters = {'C':[0.01,0.1,1], 'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr_grid2 = make_pipeline(PolynomialFeatures(degree))\n",
    "lr_grid2 = GridSearchCV(logit, param_grid = hyperparameters, scoring= scores, cv=10)\n",
    "lr_grid2.fit(X_train, y_train)\n",
    "y_pred_train, y_pred_test = lr_grid2.predict(X_train), lr_grid2.predict(X_test)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "\n",
    "print(lr_grid2.best_params_)\n",
    "print()\n",
    "\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = lr_grid2.cv_results_['mean_test_score']\n",
    "stds = lr_grid2.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, lr_grid2.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC: Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  655  5587]\n",
      " [  498 15280]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.10      0.18      6242\n",
      "          1       0.73      0.97      0.83     15778\n",
      "\n",
      "avg / total       0.69      0.72      0.65     22020\n",
      "\n",
      "Train Accuracy: 0.7164266251459712\n",
      "Test Accuracy: 0.7236603088101726\n"
     ]
    }
   ],
   "source": [
    "svml = LinearSVC()\n",
    "svml.fit(X_train, y_train)\n",
    "y_pred_train = svml.predict(X_train) \n",
    "y_pred_test = svml.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on development set:\n",
      "\n",
      "0.717 (+/-0.008) for {'C': 0.001}\n",
      "0.717 (+/-0.008) for {'C': 0.1}\n",
      "0.716 (+/-0.007) for {'C': 1.0}\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.11      0.18      6242\n",
      "          1       0.73      0.97      0.83     15778\n",
      "\n",
      "avg / total       0.69      0.72      0.65     22020\n",
      "\n",
      "[[  669  5573]\n",
      " [  511 15267]]\n",
      "Train Accuracy: 0.7170688984040483\n",
      "Test Accuracy: 0.7237057220708447\n"
     ]
    }
   ],
   "source": [
    "svml = LinearSVC()\n",
    "param_grid = {'C': [1e-3, 1e-1,1.0]}\n",
    "score = 'recall'\n",
    "\n",
    "svmlg = GridSearchCV(svml, param_grid=param_grid, scoring = score, cv =10)  \n",
    "svmlg.fit(X_train, y_train)\n",
    "y_pred_train, y_pred_test = svmlg.predict(X_train), svmlg.predict(X_test)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "\n",
    "print(svmlg.best_params_)\n",
    "print()\n",
    "\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "\n",
    "means = svmlg.cv_results_['mean_test_score']\n",
    "stds = svml.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, svmlg.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "\n",
    "print(classification_report (y_test,y_pred_test))\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1566  4771]\n",
      " [ 1116 14580]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.25      0.35      6337\n",
      "          1       0.75      0.93      0.83     15696\n",
      "\n",
      "avg / total       0.70      0.73      0.69     22033\n",
      "\n",
      "Train Accuracy: 0.744884064737006\n",
      "Test Accuracy: 0.7328098760949485\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "y_pred_train, y_pred_test = svm.predict(X_train), svm.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC algorithm was not optimized because it takes a very long time to run     \n",
    "There was a very low probability this would be the best algorithm.      \n",
    "To optimize it, the following hyperparameters would be tested:     \n",
    "{'C': [1.0, 10., 100.], 'degree':[2,3,4],'gamma':[1e-5,1e-3,1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "This is one of the models used so the feature importances was printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.24512008e-01 2.26095549e-02 2.91705283e-01 1.93740015e-02\n",
      " 1.78159945e-02 3.42193492e-03 1.88691370e-03 2.64500922e-03\n",
      " 1.29994876e-03 5.73965641e-03 4.74170905e-03 5.23707085e-03\n",
      " 5.89917544e-03 8.19120165e-03 3.36964734e-03 4.99903792e-03\n",
      " 3.56619900e-03 9.87786229e-03 2.05893627e-02 4.66280766e-03\n",
      " 3.14141510e-03 4.26932288e-03 3.64632501e-03 5.59230251e-03\n",
      " 1.19575099e-02 1.21044912e-02 1.15962160e-02 1.13977175e-02\n",
      " 1.24769277e-02 1.13534201e-02 1.15149557e-02 1.16285455e-02\n",
      " 9.54074437e-03 1.08124517e-02 1.11462628e-02 3.13559938e-03\n",
      " 8.77431096e-03 7.96809146e-03 5.64500553e-03 5.77031090e-04\n",
      " 5.07419578e-04 2.33457704e-03 1.38921055e-02 1.05221097e-02\n",
      " 9.92158889e-03 7.89401046e-04 1.92749187e-03 9.95602650e-03\n",
      " 4.72180292e-03 2.18844763e-04 8.06124927e-05 4.86355560e-03\n",
      " 5.38284007e-03 4.45859809e-03]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.43      0.45      6242\n",
      "          1       0.78      0.81      0.79     15778\n",
      "\n",
      "avg / total       0.69      0.70      0.70     22020\n",
      "\n",
      "[[ 2691  3551]\n",
      " [ 3046 12732]]\n",
      "Train Accuracy: 0.9778707668353445\n",
      "Test Accuracy: 0.7004087193460491\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_test= rf.predict(X_test)\n",
    "\n",
    "print(classification_report (y_test,y_pred_test))\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.674 (+/-0.033) for {'max_depth': 10, 'n_estimators': 5}\n",
      "0.677 (+/-0.032) for {'max_depth': 10, 'n_estimators': 10}\n",
      "0.682 (+/-0.033) for {'max_depth': 10, 'n_estimators': 20}\n",
      "0.683 (+/-0.034) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.688 (+/-0.038) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.648 (+/-0.021) for {'max_depth': 20, 'n_estimators': 5}\n",
      "0.665 (+/-0.024) for {'max_depth': 20, 'n_estimators': 10}\n",
      "0.674 (+/-0.023) for {'max_depth': 20, 'n_estimators': 20}\n",
      "0.682 (+/-0.021) for {'max_depth': 20, 'n_estimators': 50}\n",
      "0.684 (+/-0.024) for {'max_depth': 20, 'n_estimators': 100}\n",
      "0.627 (+/-0.014) for {'max_depth': 30, 'n_estimators': 5}\n",
      "0.641 (+/-0.018) for {'max_depth': 30, 'n_estimators': 10}\n",
      "0.653 (+/-0.023) for {'max_depth': 30, 'n_estimators': 20}\n",
      "0.667 (+/-0.017) for {'max_depth': 30, 'n_estimators': 50}\n",
      "0.667 (+/-0.023) for {'max_depth': 30, 'n_estimators': 100}\n",
      "0.618 (+/-0.017) for {'max_depth': 50, 'n_estimators': 5}\n",
      "0.628 (+/-0.010) for {'max_depth': 50, 'n_estimators': 10}\n",
      "0.642 (+/-0.024) for {'max_depth': 50, 'n_estimators': 20}\n",
      "0.649 (+/-0.018) for {'max_depth': 50, 'n_estimators': 50}\n",
      "0.652 (+/-0.020) for {'max_depth': 50, 'n_estimators': 100}\n",
      "0.620 (+/-0.015) for {'max_depth': 100, 'n_estimators': 5}\n",
      "0.628 (+/-0.015) for {'max_depth': 100, 'n_estimators': 10}\n",
      "0.641 (+/-0.016) for {'max_depth': 100, 'n_estimators': 20}\n",
      "0.651 (+/-0.021) for {'max_depth': 100, 'n_estimators': 50}\n",
      "0.652 (+/-0.021) for {'max_depth': 100, 'n_estimators': 100}\n",
      "0.619 (+/-0.013) for {'max_depth': 200, 'n_estimators': 5}\n",
      "0.629 (+/-0.011) for {'max_depth': 200, 'n_estimators': 10}\n",
      "0.638 (+/-0.019) for {'max_depth': 200, 'n_estimators': 20}\n",
      "0.649 (+/-0.019) for {'max_depth': 200, 'n_estimators': 50}\n",
      "0.650 (+/-0.024) for {'max_depth': 200, 'n_estimators': 100}\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.08      0.15      6337\n",
      "          1       0.73      0.98      0.83     15696\n",
      "\n",
      "avg / total       0.70      0.72      0.64     22033\n",
      "\n",
      "[[  520  5817]\n",
      " [  296 15400]]\n",
      "Train Accuracy: 0.7305672268907563\n",
      "Test Accuracy: 0.7225525348341124\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 100, 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.542 (+/-0.022) for {'max_depth': 10, 'n_estimators': 5}\n",
      "0.539 (+/-0.020) for {'max_depth': 10, 'n_estimators': 10}\n",
      "0.539 (+/-0.018) for {'max_depth': 10, 'n_estimators': 20}\n",
      "0.536 (+/-0.009) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.539 (+/-0.006) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.592 (+/-0.016) for {'max_depth': 20, 'n_estimators': 5}\n",
      "0.595 (+/-0.015) for {'max_depth': 20, 'n_estimators': 10}\n",
      "0.595 (+/-0.013) for {'max_depth': 20, 'n_estimators': 20}\n",
      "0.598 (+/-0.013) for {'max_depth': 20, 'n_estimators': 50}\n",
      "0.598 (+/-0.012) for {'max_depth': 20, 'n_estimators': 100}\n",
      "0.603 (+/-0.013) for {'max_depth': 30, 'n_estimators': 5}\n",
      "0.610 (+/-0.017) for {'max_depth': 30, 'n_estimators': 10}\n",
      "0.613 (+/-0.012) for {'max_depth': 30, 'n_estimators': 20}\n",
      "0.614 (+/-0.018) for {'max_depth': 30, 'n_estimators': 50}\n",
      "0.615 (+/-0.018) for {'max_depth': 30, 'n_estimators': 100}\n",
      "0.607 (+/-0.018) for {'max_depth': 50, 'n_estimators': 5}\n",
      "0.619 (+/-0.013) for {'max_depth': 50, 'n_estimators': 10}\n",
      "0.617 (+/-0.017) for {'max_depth': 50, 'n_estimators': 20}\n",
      "0.618 (+/-0.016) for {'max_depth': 50, 'n_estimators': 50}\n",
      "0.617 (+/-0.018) for {'max_depth': 50, 'n_estimators': 100}\n",
      "0.605 (+/-0.012) for {'max_depth': 100, 'n_estimators': 5}\n",
      "0.619 (+/-0.014) for {'max_depth': 100, 'n_estimators': 10}\n",
      "0.617 (+/-0.017) for {'max_depth': 100, 'n_estimators': 20}\n",
      "0.619 (+/-0.019) for {'max_depth': 100, 'n_estimators': 50}\n",
      "0.617 (+/-0.016) for {'max_depth': 100, 'n_estimators': 100}\n",
      "0.608 (+/-0.019) for {'max_depth': 200, 'n_estimators': 5}\n",
      "0.619 (+/-0.015) for {'max_depth': 200, 'n_estimators': 10}\n",
      "0.618 (+/-0.015) for {'max_depth': 200, 'n_estimators': 20}\n",
      "0.618 (+/-0.018) for {'max_depth': 200, 'n_estimators': 50}\n",
      "0.617 (+/-0.018) for {'max_depth': 200, 'n_estimators': 100}\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.43      0.45      6337\n",
      "          1       0.78      0.81      0.79     15696\n",
      "\n",
      "avg / total       0.69      0.70      0.69     22033\n",
      "\n",
      "[[ 2721  3616]\n",
      " [ 3005 12691]]\n",
      "Train Accuracy: 0.9783107687519452\n",
      "Test Accuracy: 0.6994962102301093\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 100, 'n_estimators': 50}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.513 (+/-0.034) for {'max_depth': 10, 'n_estimators': 5}\n",
      "0.509 (+/-0.027) for {'max_depth': 10, 'n_estimators': 10}\n",
      "0.498 (+/-0.032) for {'max_depth': 10, 'n_estimators': 20}\n",
      "0.498 (+/-0.026) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.499 (+/-0.021) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.601 (+/-0.016) for {'max_depth': 20, 'n_estimators': 5}\n",
      "0.601 (+/-0.018) for {'max_depth': 20, 'n_estimators': 10}\n",
      "0.600 (+/-0.018) for {'max_depth': 20, 'n_estimators': 20}\n",
      "0.601 (+/-0.018) for {'max_depth': 20, 'n_estimators': 50}\n",
      "0.601 (+/-0.018) for {'max_depth': 20, 'n_estimators': 100}\n",
      "0.607 (+/-0.019) for {'max_depth': 30, 'n_estimators': 5}\n",
      "0.621 (+/-0.016) for {'max_depth': 30, 'n_estimators': 10}\n",
      "0.620 (+/-0.017) for {'max_depth': 30, 'n_estimators': 20}\n",
      "0.624 (+/-0.018) for {'max_depth': 30, 'n_estimators': 50}\n",
      "0.624 (+/-0.017) for {'max_depth': 30, 'n_estimators': 100}\n",
      "0.605 (+/-0.014) for {'max_depth': 50, 'n_estimators': 5}\n",
      "0.621 (+/-0.010) for {'max_depth': 50, 'n_estimators': 10}\n",
      "0.625 (+/-0.019) for {'max_depth': 50, 'n_estimators': 20}\n",
      "0.627 (+/-0.014) for {'max_depth': 50, 'n_estimators': 50}\n",
      "0.625 (+/-0.017) for {'max_depth': 50, 'n_estimators': 100}\n",
      "0.612 (+/-0.016) for {'max_depth': 100, 'n_estimators': 5}\n",
      "0.623 (+/-0.016) for {'max_depth': 100, 'n_estimators': 10}\n",
      "0.626 (+/-0.018) for {'max_depth': 100, 'n_estimators': 20}\n",
      "0.627 (+/-0.018) for {'max_depth': 100, 'n_estimators': 50}\n",
      "0.626 (+/-0.019) for {'max_depth': 100, 'n_estimators': 100}\n",
      "0.610 (+/-0.017) for {'max_depth': 200, 'n_estimators': 5}\n",
      "0.622 (+/-0.014) for {'max_depth': 200, 'n_estimators': 10}\n",
      "0.626 (+/-0.015) for {'max_depth': 200, 'n_estimators': 20}\n",
      "0.626 (+/-0.017) for {'max_depth': 200, 'n_estimators': 50}\n",
      "0.625 (+/-0.020) for {'max_depth': 200, 'n_estimators': 100}\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.38      0.44      6337\n",
      "          1       0.78      0.86      0.81     15696\n",
      "\n",
      "avg / total       0.70      0.72      0.71     22033\n",
      "\n",
      "[[ 2437  3900]\n",
      " [ 2262 13434]]\n",
      "Train Accuracy: 0.9925108932461874\n",
      "Test Accuracy: 0.7203285980120728\n"
     ]
    }
   ],
   "source": [
    "randomforest = RandomForestClassifier()\n",
    "param_grid={'n_estimators': [5, 10, 20, 50, 100], 'max_depth': [10, 20, 30, 50, 100, 200]}\n",
    "scores = ['recall']\n",
    "\n",
    "rfg3 = GridSearchCV(randomforest, param_grid=param_grid, scoring=scores, cv =10)\n",
    "rfg3.fit(X_train, y_train)\n",
    "y_pred_train, y_pred_test = rfg3.predict(X_train), rfg3.predict(X_test)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(rfg3.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = rfg3.cv_results_['mean_test_score']\n",
    "stds = rfg3.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, rfg3.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "\n",
    "print(classification_report (y_test,y_pred_test))\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier()\n",
    "gbt.fit(X_train, y_train)\n",
    "y_pred_train = gbt.predict(X_train)\n",
    "y_pred_test= gbt.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report (y_test,y_pred_test))\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = GradientBoostingClassifier()\n",
    "param_grid = {'n_estimators': [800,1000]}\n",
    "scores = ['recall']\n",
    "\n",
    "gbtg2 = GridSearchCV(gradient, param_grid=param_grid, scoring = scores, cv =10)\n",
    "gbtg2.fit(X_train, y_train)\n",
    "y_pred_train = gbtg2.predict(X_train)\n",
    "y_pred_test= gbtg2.predict(X_test)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(gbtg2.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = gbtg2.cv_results_['mean_test_score']\n",
    "stds = gbtg2.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gbtg2.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "\n",
    "print(classification_report (y_test,y_pred_test))\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.710\n",
      "[[ 2011  4231]\n",
      " [ 2157 13621]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.32      0.39      6242\n",
      "          1       0.76      0.86      0.81     15778\n",
      "\n",
      "avg / total       0.68      0.71      0.69     22020\n",
      "\n",
      "Train Accuracy: 0.7068314519268197\n",
      "Test Accuracy: 0.7099000908265214\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB() \n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred_train = bnb.predict(X_train)\n",
    "y_pred_test= bnb.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(y_test,y_pred_test))\n",
    "print(classification_report(y_test, bnb.predict(X_test)))\n",
    "print('Train Accuracy:',metrics.accuracy_score(y_train, y_pred_train))\n",
    "print('Test Accuracy:',metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a baseline.  70% of the cases were solved, 30% not solved.  By always guessing they were solved,\n",
    "a person would be correct 70% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0      1\n",
      "0  0   6337\n",
      "1  0  15696\n",
      "      0      1\n",
      "0  2330   4007\n",
      "1  2053  13643\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      6337\n",
      "          1       0.71      1.00      0.83     15696\n",
      "\n",
      "avg / total       0.51      0.71      0.59     22033\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.37      0.43      6337\n",
      "          1       0.77      0.87      0.82     15696\n",
      "\n",
      "avg / total       0.70      0.72      0.71     22033\n",
      "\n",
      "Guess Accuracy: 0.7123859665047882\n",
      "Random Forest Accuracy: 0.7249580175191758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_guess_1 = y_test*0+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC curves are calculated to determine which alorithm is best for both precision and recall.\n",
    "\n",
    "y_pred is not a prediction of 0 or 1.  It is a probability.  Should have used different name.\n",
    "\n",
    "When SVMLinear is run with predict_proba, an error of \"SVM needs decision_function instead of predict_proba\"\n",
    "is given.  A workaround is using decision_function. SVM with the kernel = linear would have worked also\n",
    "Decision function was used for svm also, but predict_proba should have been used.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_knn = knn.predict_proba(X_test)[:, 1]\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_pred_knn)\n",
    "\n",
    "y_pred_lrg2 = lrg2.predict_proba(X_test)[:, 1]\n",
    "fpr_lrg2, tpr_lrg2, _ = roc_curve(y_test, y_pred_lrg2)\n",
    "\n",
    "y_pred_svml = svml.predict_proba(X_test)\n",
    "fpr_svml, tpr_svml, _ = roc_curve(y_test, y_pred_svml)\n",
    "\n",
    "y_pred_svmlg = svmlg.decision_function(X_test)\n",
    "fpr_svmlg, tpr_svmlg, _ = roc_curve(y_test, y_pred_svmlg)\n",
    "\n",
    "y_pred_svm = svm.decision_function(X_test)\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_pred_svm)\n",
    "\n",
    "y_pred_rfg3 = rfg3.predict_proba(X_test)[:, 1]\n",
    "fpr_rfg3, tpr_rfg3, _ = roc_curve(y_test, y_pred_rfg3)\n",
    "\n",
    "y_pred_gbtg2 = gbtg2.predict_proba(X_test)[:, 1]\n",
    "fpr_gbtg2, tpr_gbtg2, _ = roc_curve(y_test, y_pred_gbtg2)\n",
    "\n",
    "y_pred_bnb = bnb.predict_proba(X_test)[:, 1]\n",
    "fpr_bnb, tpr_bnb, _ = roc_curve(y_test, y_pred_bnb)\n",
    "\n",
    "y_guess_1 = y_test*0+1\n",
    "fpr_guess, tpr_guess, _ = roc_curve(y_test, y_guess_1)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.plot(fpr_knn, tpr_knn, label='KNN (unoptimized)', c='0.8')\n",
    "plt.plot(fpr_lrg2, tpr_lrg2, label='Logistic Regression',c='0.8')\n",
    "plt.plot(fpr_svmlg, tpr_svmlg, label='SVM Linear',c='0.8')\n",
    "plt.plot(fpr_svm, tpr_svm, label='SVM (unoptimized)',c='0.8')\n",
    "plt.plot(fpr_rfg3, tpr_rfg3, label='Random Forest',c='b')\n",
    "plt.plot(fpr_gbtg2, tpr_gbtg2, label='Gradient Boosting',c='r')\n",
    "plt.plot(fpr_bnb, tpr_bnb, label='Naive Bayes (Bernoulli)',c= '0.8')\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC scores\n",
      "KNN: 0.647\n",
      "Linear Regression: 0.683\n",
      "Linear SVC: 0.683\n",
      "SVC: 0.679\n",
      "Random Forest: 0.675\n",
      "Gradient Boosting: 0.721\n",
      "Naive Bayes: 0.662\n"
     ]
    }
   ],
   "source": [
    "print ('AUC scores')\n",
    "print ('KNN: {}'.format(round(roc_auc_score(y_test, y_pred_knn),3)))\n",
    "print ('Linear Regression: {}'.format(round(roc_auc_score(y_test, y_pred_lrg2),3)))\n",
    "print ('Linear SVC: {}'.format(round(roc_auc_score(y_test, y_pred_svmlg),3)))\n",
    "print ('SVC: {}'.format(round(roc_auc_score(y_test, y_pred_svm),3)))\n",
    "print ('Random Forest: {}'.format(round(roc_auc_score(y_test, y_pred_rfg3),3)))\n",
    "print ('Gradient Boosting: {}'.format(round(roc_auc_score(y_test, y_pred_gbtg2),3)))\n",
    "print ('Naive Bayes: {}'.format(round(roc_auc_score(y_test, y_pred_bnb),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get recall metric   \n",
    "Need to overwrite y predictions in memory that hold probabilities   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_KNN= knn.predict(X_test)\n",
    "y_pred_lrg2= lrg2.predict(X_test)\n",
    "y_pred_svmlg= svmlg.predict(X_test)\n",
    "y_pred_svm= svm.predict(X_test)\n",
    "y_pred_rfg3= rfg3.predict(X_test)\n",
    "y_pred_gbtg2= gbtg2.predict(X_test)\n",
    "y_pred_bnb= bnb.predict(X_test)\n",
    "\n",
    "print ('Recall scores')\n",
    "print ('KNN: {}'.format(round(recall_score(y_test, y_pred_KNN,pos_label=0),3)))\n",
    "print ('Linear Regression: {}'.format(round(recall_score(y_test, y_pred_lrg2,pos_label=0),3)))\n",
    "print ('Linear SVC: {}'.format(round(recall_score(y_test, y_pred_svmlg,pos_label=0),3)))\n",
    "print ('SVC: {}'.format(round(recall_score(y_test, y_pred_svm,pos_label=0),3)))\n",
    "print ('Random Forest: {}'.format(round(recall_score(y_test, y_pred_rfg3, pos_label=0),3)))\n",
    "print ('Gradient Boosting: {}'.format(round(recall_score(y_test, y_pred_gbtg2, pos_label=0),3)))\n",
    "print ('Naive Bayes: {}'.format(round(recall_score(y_test, y_pred_bnb, pos_label=0),3)))\n",
    "print()\n",
    "print ('Baseline: {}'.format(round(recall_score(y_test, y_guess_1, pos_label=0),3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = list(zip(X,rfg3.best_estimator_.feature_importances_))\n",
    "cols = X.columns\n",
    "dftable = pd.DataFrame(table, columns=['cols','Coef'])\n",
    "dftable.sort_values(by='Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the values of one homicide case for a test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns\n",
    "vals = X.iloc[0]\n",
    "table = list(zip(cols,vals))\n",
    "dftable = pd.DataFrame(table, columns=['Features','vals'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "laura = dftable.iloc[:,1]\n",
    "laura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a list of column names with their index to identify which values to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Vic_Age')\n",
      "(1, 'Vic_Count')\n",
      "(2, 'Year')\n",
      "(3, 'vic_sex')\n",
      "(4, 'agent_type')\n",
      "(5, 'msa[T.Baltimore-Towson, MD]')\n",
      "(6, 'msa[T.Birmingham-Hoover, AL]')\n",
      "(7, 'msa[T.Boston-Cambridge-Quincy, MA-NH]')\n",
      "(8, 'msa[T.Bridgeport-Stamford-Norwalk, CT]')\n",
      "(9, 'msa[T.Chicago-Naperville-Joliet, IL-IN-WI]')\n",
      "(10, 'msa[T.Dallas-Fort Worth-Arlington, TX]')\n",
      "(11, 'msa[T.Detroit-Warren-Livonia, MI]')\n",
      "(12, 'msa[T.Houston-Sugar Land-Baytown, TX]')\n",
      "(13, 'msa[T.Los Angeles-Long Beach, CA]')\n",
      "(14, 'msa[T.Memphis, TN-MS-AR]')\n",
      "(15, 'msa[T.Miami-Fort Lauderdale, FL]')\n",
      "(16, 'msa[T.New Orleans-Metairie-Kenner, LA]')\n",
      "(17, 'msa[T.New York-New Jersey-Long Island, NY-NJ]')\n",
      "(18, 'msa[T.Other]')\n",
      "(19, 'msa[T.Philadelphia-Camden-Wilmington, PA-NJ-DE]')\n",
      "(20, 'msa[T.Riverside-San Bernardino, CA]')\n",
      "(21, 'msa[T.San Francisco-Oakland-Fremont, CA]')\n",
      "(22, 'msa[T.St. Louis, MO-IL]')\n",
      "(23, 'msa[T.Washington-Arlington-Alexandria, DC-VA-MD-WV]')\n",
      "(24, 'Month[T.August]')\n",
      "(25, 'Month[T.December]')\n",
      "(26, 'Month[T.February]')\n",
      "(27, 'Month[T.January]')\n",
      "(28, 'Month[T.July]')\n",
      "(29, 'Month[T.June]')\n",
      "(30, 'Month[T.March]')\n",
      "(31, 'Month[T.May]')\n",
      "(32, 'Month[T.November]')\n",
      "(33, 'Month[T.October]')\n",
      "(34, 'Month[T.September]')\n",
      "(35, 'Vic_Race[T.Asian or Pacific Islander]')\n",
      "(36, 'Vic_Race[T.Black]')\n",
      "(37, 'Vic_Race[T.White]')\n",
      "(38, 'Weapon[T.Blunt object - hammer, club, etc]')\n",
      "(39, 'Weapon[T.Drowning]')\n",
      "(40, 'Weapon[T.Explosives]')\n",
      "(41, 'Weapon[T.Fire]')\n",
      "(42, 'Weapon[T.Firearm, type not stated]')\n",
      "(43, 'Weapon[T.Handgun - pistol, revolver, etc]')\n",
      "(44, 'Weapon[T.Knife or cutting instrument]')\n",
      "(45, 'Weapon[T.Narcotics or drugs, sleeping pills]')\n",
      "(46, 'Weapon[T.Other gun]')\n",
      "(47, 'Weapon[T.Other or type unknown]')\n",
      "(48, 'Weapon[T.Personal weapons, includes beating]')\n",
      "(49, 'Weapon[T.Poison - does not include gas]')\n",
      "(50, 'Weapon[T.Pushed or thrown out window]')\n",
      "(51, 'Weapon[T.Rifle]')\n",
      "(52, 'Weapon[T.Shotgun]')\n",
      "(53, 'Weapon[T.Strangulation - hanging]')\n"
     ]
    }
   ],
   "source": [
    "for ele in enumerate(cols):\n",
    "    print (ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case is a 44 year old woman killed in Seattle in August, the Seattle police investigated, by poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "laura[0] = 44\n",
    "laura[2] = 2018\n",
    "laura[3] = 1\n",
    "laura[18] = 1\n",
    "laura[23] = 0\n",
    "laura[24] = 1\n",
    "laura[34] = 0\n",
    "laura[36] = 0\n",
    "laura[37] = 1\n",
    "laura[42] = 0\n",
    "laura[47] = 0\n",
    "laura[49] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = list(zip(cols,laura))\n",
    "dftable1 = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1     2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23  \\\n",
       "1  44  0  2018  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0   \n",
       "\n",
       "  24 ... 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  \\\n",
       "1  1 ...  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0   \n",
       "\n",
       "  52 53  \n",
       "1  0  0  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laura_vals= dftable1\n",
    "trans = laura_vals.T\n",
    "laura = trans.drop([0, 0])\n",
    "laura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfg3.predict_proba(laura)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
